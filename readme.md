# OS CHALLENGE (2021)
## INTRO:

This repository is group “solo-leveling”s solution to the OS Challenge of DTU’s Introduction to Operating System course (02159).  
The group consists of a single member: August Falck Kreinert Valentin (S194802).  
The challenge has been worked on throughout the duration of the course, from 23rd September 2021, to finalizing experiments and report the 19th November 2021. The progress was slow at first, but pace was picked up as I became more used to C and its pointer syntax.  
  
The OS challenge is to develop a server which responds to client requests of a certain kind.  
The requests are 32-byte hashes, generated by encrypting a 64-bit integer key, using the SHA-256 hashing algorithm. In addition to the hash, the request contains a range of 64-bit integer type, which the corresponding key is located in. The request also comes with a priority, signaling if the task is urgent.  
  
The purpose of the challenge is to emulate how an operating system should handle tasks of differing priority, size, in both small bursts and over a duration of time. How well our implementation works is measured as a score, a weighted average of time used for each request, with priority serving as the weight. Thus, a lower score means a better implementation.

When a working server was achieved, experiments were carried out to see if we could improve the implementation. The experiments’ reasoning, hypotheses and results are presented later. In the following section, the structure of the repository is explained.

## Structure

### Code structure  
The code has been split into relevant files;
Main loop is located in servercode.c
priority list is located in prioritycode.c
Structure for priority list is located in auxstructs.h
hashmap is located in hashmap.c
Decryption code is located in hashcode.c

### Folder structure  
Our header files (.h) are located in the “include” folder.  
Our compiled binary files (.o) are located in the “obj” folder. Our server executable is in the root directory, along with all the C source-code files (.c), together with the Makefile.  
All remnants of past code and text is in the “OLD misc.” folder.  
Lastly, our readme.md is also located in the root directory.

### Branch structure
We have the ‘main’ branch which uses the FINAL SERVER implementation.  
Then we have the BASIC branch, which houses the BASIC server implementation, used before experimenting with different properties.

### Experiment structure
We have a completed server with all the necessary features.
Sets up a socket, accepts requests, sort-inserts requests into a linked list, multithreaded decryption of requests, storing previous decryption results & utilizing them.

However, we want it to be as fast as possible.
To do so, we will perform experiments on the effect of 5 properties:
NUMBER OF THREADS, HASHMAP PROBING METHOD, HASHMAP SIZE, SLEEP BETWEEN REQUESTS, WHEN TO CHECK HASHMAP.

All experiments will be performed using 2 different setups for the client requests, 3 times each:
SETUP 1: IP=192.168.101.10 PORT=5003 SEED=5041 TOTAL=100 START=1 DIFFICULTY=30000000(3 \* 10 \*\* 7) REP=20 DELAY=750000 LAMBDA=1.5  
SETUP 2: IP=192.168.101.10 PORT=5003 SEED=0 TOTAL=100 START=0 DIFFICULTY=3000000(3 \* 10 \*\* 6) REP=40 DELAY=100000 LAMBDA=6
The result of the experiment is then the average of each setup.
In addition, each experiment will only modify one of the properties of our basic working server.
Thus, we can compare how important a modification would be.
To give a sense of how good an implementation is, we calculate the percentage difference of the implementation versus the basic implementation as such:
%diff = Percentage difference = 100 \* (MEAN subtracted by BASICMEAN) divided by BASIC.  
Meaning -100% would be if an implementation had a mean score of 0, and 50% would be an implementation with mean score of 1.5 \* BASIC’s mean = 110 400 106 (in setup 1).

The basic server has the following values for each property:
(BRANCH NAME: BASIC)
(GIT HASH: 9567193)
NUMBER OF THREADS = 3;
PROBING METHOD = quadratic;
HASHMAP SIZE = 97;
SLEEP BETWEEN REQUESTS = 0us;
HASHMAP CHECK = each individual decryption thread, before decryption.
Results:
| BASIC | SETUP 1 |  SETUP 2|
|--|--|--|
| Measurement 1 | 77 663 000 | 6 327 000 |
| Measurement 2 | 75 666 000 | 4 110 000 |
| Measurement 3 | 67 470 000 | 4 737 000 |
| MEAN  | 73 600 000 | 5 058 000 |
| %diff | 0% | 0% |

#### NUMBER OF THREADS:
The number of threads determines how many threads we will use to decrpy the hashing requests from the client at one time.
Our hypothesis is that parallelism will serve to speed up the process, but too many threads will slow it down as they have to share mutexes for the hashmap and priority list. Possibly, 3 threads will be ideal as there are four CPU cores available (as BASIC implementation has). 3 decryption threads would mean 4 active threads, as the main process is handling acceptance of client requests. We expect this to be best, as it allows one core to handle one task without having to switch around.  

#### RESULTS
| THREADS = 1 | SETUP 1 |  SETUP 2|
|--|--|--|
| Measurement 1 | 329 787 000 | 19 266 000 |
| Measurement 2 | 334 785 000 | 18 605 000 |
| Measurement 3 | 322 946 000 | 18 294 052 |
| MEAN  | 329 173 000 | 18 721 000 |
| %diff | 347%  | 270% |

| THREADS =  10 | SETUP 1 |  SETUP 2|
|--|--|--|
| Measurement 1 | 48 715 000 | 2 586 000 |
| Measurement 2 | 46 450 000 | 2 647 000 |
| Measurement 3 | 46 232 000 | 4 492 052 |
| MEAN  | 47 132 593 | 3 242 000 |
| %diff | -36%  | -36% |

| THREADS = 50 | SETUP 1 |  SETUP 2|
|--|--|--|
| Measurement 1 | 61 743 000 | 3 940 000 |
| Measurement 2 | 60 639 000 | 1 799 000 |
| Measurement 3 | 60 088 000 | 2 003 052 |
| MEAN  | 60 824 000 | 2 580 000 |
| %diff | -17%  | -49% |

It is apparent that 10 threads is optimal. Perhaps this is due to the OS handling other tasks than the server and client, and therefore increasing the thread number will, in a way, give the server higher priority compared to other processes.


#### HASHMAP PROBING METHOD:
The hashmap probing method decides how we iterate through the hashmap when getting and putting keys and values.

#### RESULTS:
| LINEAR STEP = 1 | SETUP 1 |  SETUP 2|
|--|--|--|
| Measurement 1 | 76 810 000 | 4 736 000 |
| Measurement 2 | 77 584 000 | 4 244 000 |
| Measurement 3 | 66 223 000 | 4 801 000 |
| MEAN  | 73 876 000 | 4 594 000 |
| %diff | 0%  | -9% |

| LINEAR STEP = 3 | SETUP 1 |  SETUP 2|
|--|--|--|
| Measurement 1 | 82 670 000 | 3 739 000 |
| Measurement 2 | 67 883 000 | 5 966 000 |
| Measurement 3 | 67 556 000 | 3 398 000 |
| MEAN  | 72 703 000 | 4 368 000 |
| %diff | -1%  | -9% |

#### HASHMAP SIZE:
The hashmap size determines how many key/value pairs we store at a time, and also how many attempts we will make to find a key.

#### Results:
| HASHMAP SIZE = 37 | SETUP 1 |  SETUP 2|
|--|--|--|
| Measurement 1 | 81 399 000 | 4 321000 |
| Measurement 2 | 79 106 000 | 5 131 000 |
| Measurement 3 | 84 779 000 | 6 334 052 |
| MEAN  | 81 762 000 | 5 262 000 |
| %diff | 11%  | 4% |

| THREADS = 227 | SETUP 1 |  SETUP 2|
|--|--|--|
| Measurement 1 | 76 691 000 | 4 621 000 |
| Measurement 2 | 80 242 000 | 4 814 000 |
| Measurement 3 | 76 778 000 | 4 195 052 |
| MEAN  | 77 904 000 | 4 543 000 |
| %diff | 5%  | -10% |

#### SLEEP BETWEEN REQUESTS:
Sleeping between requests seems counterintuitive, but may make sense. 
If we are accepting requests so quickly, that old requests are getting backlogged, then their time-delay will rise while we only deal with recent requests.
Waiting means that we will accept requests, so the timing between completing and accepting new requests matches.


#### Results:
| SLEEP 0.5s between requests | SETUP 1 |  SETUP 2|
|--|--|--|
| Measurement 1 | 76 618 000 | 21 598 000 |
| Measurement 2 | 76 933 000 | 21 464 000 |
| Measurement 3 | 67 899 000 | 21 441 000 |
| MEAN  | 74 483 000 | 21 501 000 |
| %diff | 1%  | 325% |

| SLEEP 1.5s between (NUM OF THREADS) requests | SETUP 1 |  SETUP 2|
|--|--|--|
| Measurement 1 | 82 386 | 20 866 000 |
| Measurement 2 | 83 642 000 | 20 821 000 |
| Measurement 3 | 84 883 000 | 20 080 000 |
| MEAN  | 83 637 000 | 20 922 000 |
| %diff | 13%  | 313% |


#### HASHMAP CHECK:
This property is regarding when to Check the hashmap if the key exists. This experiment will look at what happens when loading the requests, instead of in the threads that decrypt. This way, there would be less fighting over the hashmap mutex (only once per request decryption, when inserting the key, rather than twice, when looking for the key). Also means that if we find the key early, we avoid the overhead of adding it as a node for the decryption threads to find
#### Results:
| HASHMAP CHECK = at request acceptance | SETUP 1 | SETUP 2|
|--|--|--|
| Measurement 1 | 79 027 000 | 4 657 000 |
| Measurement 2 | 79 115 000 | 3 677 000 |
| Measurement 3 | 77 424 000 | 4 287 052 |
| MEAN  | 78 522 000 | 4 207 000 |
| %diff | 6%  | -16% |


It is important to note that the measurements show the mean, and we don’t see the distribution; when running this implementation, scores as low as 1500 appear. This could only mean that in situations of high repetition of hashes, checking early pays off. However, since acceptance of requests now takes more time, as we look through the hashmap, it results in a small deficit in configurations of low (20%) repetition, such as Setup 1.  
Thus, one would expect Setup 2 to show a significant speed increase, but this is not completely the case. As the difficulty is a magnitude 10 lower than Setup 1, requests are quickly solved. Ergo, a slower rate of request-acceptance is also negative for this configuration. To conclude, this implementation is a significant bonus if: requests are difficult AND have high repetition.


#### FINAL SERVER IMPLEMENTATION:
For the final server implementation, we use the results of our experiments to improve upon the basic version.  
From our experiments, we have learnt that we should use 10 threads, 3-step linear probing, larger hashmap = 227, no sleeping between requests, and checking the hashmap before adding a request to the priority list.
#### RESULTS:
| FINAL IMPLEMENTATION | SETUP 1 |  SETUP 2|
|--|--|--|
| Measurement 1 | 47 014 000 | 3 690 000 |
| Measurement 2 | 45 569 000 | 4 501 000 |
| Measurement 3 | 41 651 000 | 4 070 052 |
| MEAN  | 44 745 000 | 4 087 000 |
| %diff | -39%  | -19% |

#### Conclusion:
We improved upon the basic implementation, but the increase of threads did most of the job.
In addition,  we optimized for 2 setups, but it might not be suited for configurations with many difficult tasks (such as run-client-final.sh, which has 1000 requests), or configurations with easy tasks, where the overhead of sort inserting a linked list and checking a hashmap is too expensive.

#### Discussion:  
Future experiment ideas:  
A- have threads handle different parts of the start-to-end encryption space, instead of handling individual requests..  
B- Multithreaded acceptance of server requests. Alternatively, using a thread of checking the hashmap, letting the main thread accept more requests  
C- further experiment with number of threads, find which value is the true optimal between 3 to 50 threads.  
D- improve experiment consistency/’trustworthiness’ by running the code on a native Linux computer, with server and client as the only 2 active processes.
E- Change acceptance of requests to only happen when a thread is done. This way, we won't have a backlog of requests piling up. This modification only matters if the timer on requests start when we accept, and *not* when the client is ready to send it.


